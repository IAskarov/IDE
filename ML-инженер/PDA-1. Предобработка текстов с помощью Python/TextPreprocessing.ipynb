{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"bXTFLI4NF-LR"},"source":["## Этапы (простой) обработки текста\n","\n","<img src=\"images/textm.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rgf1SvTRF-LS"},"source":["\n","## Декодирование\n","\n","\n","**Def.**  \n","перевод последовательности байт в последовательность символов\n","\n","* Распаковка  \n","*plain/.zip/.gz/...*\n","* Кодировка  \n","*ASCII/utf-8/Windows-1251/...*\n","* Формат  \n","*csv/xml/json/doc...*\n","\n","Кроме того: что такое документ?\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I3xU0-PjF-LS"},"source":["## Разбиение на токены\n","**Def.**  \n","разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n","Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n","\n","\n","*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n","\n","\n","**Проблемы:**  \n","* example@example.com, 127.0.0.1\n","* С++, C#\n","* York University vs New York University\n","* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n","Альтернатива: n-граммы"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1719,"status":"ok","timestamp":1631048074715,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"},"user_tz":-180},"id":"BTJ5nrYYF-LT","outputId":"7d3bd7ab-d9ec-4f33-98df-2d1a4688b602"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1631048074720,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"},"user_tz":-180},"id":"Olg2Ah-SF-LV","outputId":"33cd3cf8-dfdf-4f0f-822b-a322096f35b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Трисия\n","любила\n","Нью\n","-\n","Йорк\n",",\n","поскольку\n","любовь\n","к\n","Нью\n","-\n","Йорку\n","могла\n","положительно\n","повлиять\n","на\n","ее\n","карьеру\n",".\n"]}],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","\n","s = \"Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.\"\n","\n","tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n","for t in tokenizer.tokenize(s): \n","    print(t)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"buEIu-_CF-LV"},"source":["## Стоп-слова\n","**Def.**  \n","Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZL-XwB9F-LW"},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","\n","print(\" \".join(stopwords.words(\"russian\")[1:20]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aXzzCadWF-LW"},"source":["Проблема: “To be or not to be\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"M2vMM3qnF-LW"},"source":["## Нормализация\n","**Def.**  \n","Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n","\n","Подходы  \n","* сформулировать набор правил, по которым преобразуется токен  \n","Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n","* явно хранить связи между токенами (WordNet – Princeton)  \n","машина → автомобиль, Windows 6→ window"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INpfX6wqF-LX"},"outputs":[],"source":["s = \"Нью-Йорк\"\n","s1 = s.lower()\n","print(s1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa4NDjrIF-LX"},"outputs":[],"source":["import re\n","s2 = re.sub(r\"\\W\", \"\", s1, flags=re.U)\n","print(s2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESECQgj2F-LY"},"outputs":[],"source":["s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n","print(s3)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mxgUV4UbF-LY"},"source":["## Стемминг и Лемматизация\n","**Def.**  \n","Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n","* Stemming – с помощью простых эвристических правил\n","  * Porter (Cambridge – 1980)\n","        5 этапов, на каждом применяется набор правил, таких как\n","            sses → ss (caresses → caress)\n","            ies → i (ponies → poni)\n","\n","  * Lovins (1968)\n","  * Paice (1990)\n","  * другие\n","* Lemmatization – с использованием словарей и морфологического анализа\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RgDqrYkdF-LZ"},"source":["## Стемминг"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgpAl1MJF-LZ"},"outputs":[],"source":["from nltk.stem.snowball import PorterStemmer\n","from nltk.stem.snowball import RussianStemmer\n","\n","\n","s = PorterStemmer()\n","print(s.stem(\"Tokenization\"))\n","print(s.stem(\"stemming\"))\n","\n","r = RussianStemmer()\n","print(r.stem(\"Авиация\"))\n","print(r.stem(\"национальный\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xfUJXVE4F-LZ"},"source":["**Наблюдение**  \n","для сложных языков лучше подходит лемматизация"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6nwjaX0zF-La"},"source":["## Лемматизация"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq8bFg_DF-La"},"outputs":[],"source":["# !pip install pymorphy2       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSekO34BF-La"},"outputs":[],"source":["import pymorphy2\n","\n","\n","morph = pymorphy2.MorphAnalyzer()\n","print(morph.parse(\"думающему\")[0].normal_form)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xc6Z0UY9F-La"},"source":["## Heaps' law\n","Эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n","\n","$$\n","M = k T^\\beta, \\;M \\text{ -- размер словаря}, \\; T \\text{ -- количество слов в корпусе}\n","$$\n","$$\n","30 \\leq k \\leq 100, \\; b \\approx 0.5\n","$$\n","\n","<img src=\"images/dim.png\">\n","<img src=\"images/heaps.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FzDJlw5OF-Lb"},"source":["## Представление документов\n","**Boolean Model.** Присутствие или отсутствие слова в документе  \n","**Bag of Words.** Порядок токенов не важен  \n","\n","*Погода была ужасная, принцесса была прекрасная.\n","Или все было наоборот?*\n","\n","Координаты\n","* Мультиномиальные: количество токенов в документе\n","* Числовые: взвешенное количество токенов в документе"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7TlHOiiCF-Lb"},"source":["## Zipf's law\n","Эмпирическая закономерность распределения частоты слов естественного языка\n","\n","$t_1, \\ldots, t_N$ - токены, отранжированные по убыванию частоты\n","   \t\n","$f_1, \\dots, f_N$ - соответствующие частоты\n","\n","**Закон Ципфа**\n","\t$$\n","\tf_i = \\frac{c}{i^k}\n","\t$$\t\n","\t\n","\tЧто еще? Посещаемость сайтов, количество друзей, население городов...\n","<img src=\"images/zipf.png\">\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0    [авторитет, и, дружба, вода, и, огонь, вещи, р...\n","1    [нет, силы, более, могучей, чем, знание, челов...\n","2    [истинная, сила, человека, не, в, порывах, а, ...\n","3           [величайшее, богатство, народа, его, язык]\n","dtype: object\n"]}],"source":["import pandas as pd\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","s = pd.Series([\n","     \"Авторитет и дружба вода и огонь, вещи разнородные и враждебные; равенство условие дружбы.\",\n","     \"Нет силы более могучей, чем знание; человек, вооружённый знанием, непобедим.\",\n","     \"Истинная сила человека не в порывах, а в нерушимом спокойствии.\",\n","     \"Величайшее богатство народа его язык.\"\n","     ],\n","    dtype=\"string\"\n",")\n","\n","# Код преобразования\n","import string\n","\n","# Создаем множество из символов пунктуации\n","exclude = set(string.punctuation)\n","\n","# Заменяем символы пунктуации на пустые строки\n","new_s = s.str.replace(f'[{string.punctuation}]', '', regex=True)\n","new_s = new_s.str.lower()\n","\n","new_s = new_s.str.rstrip()\n","new_s = new_s.str.lstrip()\n","\n","new_s = new_s.str.split()\n","\n","print(new_s)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["documents = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>awesome</th>\n","      <th>funny</th>\n","      <th>hate</th>\n","      <th>it</th>\n","      <th>like</th>\n","      <th>love</th>\n","      <th>movie</th>\n","      <th>nice</th>\n","      <th>one</th>\n","      <th>this</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n","0        0      1     0   1     1     0      1     0    0     1    0\n","1        0      0     1   0     0     0      1     0    0     1    0\n","2        1      0     0   1     1     0      0     0    0     1    1\n","3        0      0     0   1     0     1      0     1    1     0    0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","count_vectorizer = CountVectorizer()\n","\n","# Создаем the Bag-of-Words модель\n","bag_of_words = count_vectorizer.fit_transform(documents)\n","\n","# Отобразим Bag-of-Words модель как DataFrame\n","feature_names = count_vectorizer.get_feature_names_out()\n","pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["<zip at 0x264ee1c8340>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.util import ngrams\n","text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n","tokenized = text.split()\n","bigrams = ngrams(tokenized, 2)\n","bigrams"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>awesome</th>\n","      <th>funny</th>\n","      <th>hate</th>\n","      <th>it</th>\n","      <th>like</th>\n","      <th>love</th>\n","      <th>movie</th>\n","      <th>nice</th>\n","      <th>one</th>\n","      <th>this</th>\n","      <th>was</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.571848</td>\n","      <td>0.000000</td>\n","      <td>0.365003</td>\n","      <td>0.450852</td>\n","      <td>0.000000</td>\n","      <td>0.450852</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.365003</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.702035</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.553492</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.448100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.539445</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.344321</td>\n","      <td>0.425305</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.344321</td>\n","      <td>0.539445</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.345783</td>\n","      <td>0.000000</td>\n","      <td>0.541736</td>\n","      <td>0.000000</td>\n","      <td>0.541736</td>\n","      <td>0.541736</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    awesome     funny      hate        it      like      love     movie  \\\n","0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n","1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n","2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n","3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n","\n","       nice       one      this       was  \n","0  0.000000  0.000000  0.365003  0.000000  \n","1  0.000000  0.000000  0.448100  0.000000  \n","2  0.000000  0.000000  0.344321  0.539445  \n","3  0.541736  0.541736  0.000000  0.000000  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n","tfidf_vectorizer = TfidfVectorizer()\n","values = tfidf_vectorizer.fit_transform(document)\n","# Show the Model as a pandas DataFrame\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","pd.DataFrame(values.toarray(), columns = feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"TextPreprocessing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}
